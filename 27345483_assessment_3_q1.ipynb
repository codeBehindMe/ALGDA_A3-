{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A. Document Clustering\n",
    "In this part, you solve a document clustering problem using unsupervised learning algorithms (i.e., soft and hard Expectation Maximization for document clustering.\n",
    "\n",
    "## Question 1\n",
    "### i.\n",
    "See report.\n",
    "\n",
    "### ii.\n",
    "Implement the hard-EM (you derived above) and soft-EM (derived in Chapter 5 of Module 4). Please provide enough comments in your submitted code. Hint. If it helps, feel free to base your code on the provided code for EM algorithm for GMM in Activity 4.1 or the codebase provided in the Moodle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "read.data <- function(file.name = './assessments_datasets/Task3A.txt', sample.size = 1000, seed = 100, pre.proc = TRUE, spr.ratio = 0.90) {\n",
    "    # INPUTS:\n",
    "    ## file.name: name of the input .txt file\n",
    "    ## sample.size: if == 0  reads all docs, otherwise only reads a subset of the corpus\n",
    "    ## seed: random seed for sampling (read above)\n",
    "    ## pre.proc: if TRUE performs the preprocessing (recommended)\n",
    "    ## spr.ratio: is used to reduce the sparcity of data by removing very infrequent words\n",
    "    # OUTPUTS:\n",
    "    ## docs: the unlabled corpus (each row is a document)\n",
    "    ## word.doc.mat: the count matrix (each rows and columns corresponds to words and documents, respectively)\n",
    "    ## label: the real cluster labels (will be used in visualization/validation and not for clustering)\n",
    "\n",
    "    # Read the data\n",
    "    text <- readLines(file.name)\n",
    "    # select a subset of data if sample.size > 0\n",
    "    if (sample.size > 0) {\n",
    "        set.seed(seed)\n",
    "        text <- text[sample(length(text), sample.size)]\n",
    "    }\n",
    "    ## the terms before the first '\\t' are the lables (the newsgroup names) and all the remaining text after '\\t' are the actual documents\n",
    "    docs <- strsplit(text, '\\t')\n",
    "    # store the labels for evaluation\n",
    "    labels <- unlist(lapply(docs, function(x) x[1]))\n",
    "    # store the unlabeled texts    \n",
    "    docs <- data.frame(unlist(lapply(docs, function(x) x[2])))\n",
    "\n",
    "\n",
    "    library(tm)\n",
    "    # create a corpus\n",
    "    docs <- DataframeSource(docs)\n",
    "    corp <- Corpus(docs)\n",
    "\n",
    "    # Preprocessing:\n",
    "    if (pre.proc) {\n",
    "        corp <- tm_map(corp, removeWords, stopwords(\"english\")) # remove stop words (the most common word in a language that can be find in any document)\n",
    "        corp <- tm_map(corp, removePunctuation) # remove pnctuation\n",
    "        corp <- tm_map(corp, stemDocument) # perform stemming (reducing inflected and derived words to their root form)\n",
    "        corp <- tm_map(corp, removeNumbers) # remove all numbers\n",
    "        corp <- tm_map(corp, stripWhitespace) # remove redundant spaces \n",
    "    }\n",
    "    # Create a matrix which its rows are the documents and colomns are the words. \n",
    "    dtm <- DocumentTermMatrix(corp)\n",
    "    ## reduce the sparcity of out dtm\n",
    "    dtm <- removeSparseTerms(dtm, spr.ratio)\n",
    "    ## convert dtm to a matrix\n",
    "    word.doc.mat <- t(as.matrix(dtm))\n",
    "\n",
    "    # Return the result\n",
    "    return(list(\"docs\" = docs, \"word.doc.mat\" = word.doc.mat, \"labels\" = labels))\n",
    "}\n",
    "\n",
    "intern_logSum <- function(v) {\n",
    "    ## --- helper function ------------------------------------------------------------------ \n",
    "    # Input:    logA1, logA2 ... logAn\n",
    "    # Output:   log(A1+A2+...+An)\n",
    "    #\n",
    "    # This function is needed to prevent numerical overflow/underflow when working with small numbers, \n",
    "    # because we can easily get small numbers by multiplying p1 * p2 * ... * pn (where 0 <= pi <= 1 are probabilities).   \n",
    "    #\n",
    "    # Example: Suppose we are interested in p1*p2*p3 + q1*q2+q3 where all numbers are probabilities \\in [0,1]\n",
    "    #          To prevent numerical errors, we do the computation in the log space and convert the result back using the exp function \n",
    "    #          Hence our approach is to form the vector v = [log(p1)+log(p2)+log(p3) , log(q1)+log(q2)+log(q3)] \n",
    "    #          Then get the results by: exp(logSum(v))\n",
    "\n",
    "    m = max(v)\n",
    "    return(m + log(sum(exp(v - m))))\n",
    "}\n",
    "\n",
    "\n",
    "intern_expectation <- function(featureVector, K, theta, gamma,hard=FALSE) {\n",
    "    # This internal function implements the expectation step for the EM for GMMs.\n",
    "\n",
    "    # ARGS #\n",
    "    ########\n",
    "    # featureVector ~ Count / TF-IDF vector as matrix.\n",
    "    # K ~ Number of expected clusters.\n",
    "    # theta ~ List object container with mixing component \"phi\" and cluster centroids \"mu\".\n",
    "    # gamma ~ posterior estimates for p(Z|X,thetaOld)\n",
    "    # axis ~ Indicate whether the unique words are given rowwise or column wise. 1 Indicates rowwise. \n",
    "    # hard ~ Whether the cluster assignment is probabilistic or absolute. If hard = TRUE, then cluster assignments are made by assignining the cluster of the highest probability of generation. i.e. argmax ln(p(Z|X,theta)).\n",
    "\n",
    "    # RETURN #\n",
    "    ##########\n",
    "    # Returns posterior estmates for theta as matrix.\n",
    "\n",
    "\n",
    "    # recast feature vector to local variable.\n",
    "    fv_ <- featureVector;\n",
    "\n",
    "    # get number of documents and number of unique words\n",
    "    \n",
    "    N <- dim(fv_)[2] # number of documents.\n",
    "    K <- K # number of clusters.\n",
    "\n",
    "\n",
    "    # BEGIN #\n",
    "    #########\n",
    "\n",
    "    for (n in 1:N) {\n",
    "        # for each document.\n",
    "        \n",
    "        for (k in 1:K) {\n",
    "            # for each cluster.\n",
    "            ## calculate the posterior based on the estimated mu and rho in the \"log space\"\n",
    "            gamma[n, k] <- log(theta$phi[k, 1]) + sum(fv_[, n] * log(theta$mu[k,]))\n",
    "        }\n",
    "\n",
    "        # normalise to  1 in the logspace\n",
    "        logZ_ = intern_logSum(gamma[n,])\n",
    "        gamma[n,] = gamma[n,] - logZ_\n",
    "\n",
    "    }\n",
    "\n",
    "    # convert them back from log\n",
    "    gamma <- exp(gamma)\n",
    "\n",
    "    # if hard cluster make max gamma 1 and else 0\n",
    "    if (hard) {\n",
    "        maxGamma_ <- gamma == apply(gamma, 1, max) # find cluster with max probl.\n",
    "        gamma[maxGamma_] <- 1 # set max prob cluster to 1\n",
    "        gamma[!maxGamma_] <- 0 # 0 for all others.\n",
    "    }\n",
    "\n",
    "    return(gamma);\n",
    "\n",
    "}\n",
    "\n",
    "intern_maximisation <- function(featureVector,gamma,theta,K) {\n",
    "\n",
    "    # This internal function implements the expectation step for the EM for GMMs.\n",
    "\n",
    "    # ARGS #\n",
    "    ########\n",
    "    # featureVector ~ Count / TF-IDF vector as matrix.\n",
    "    # K ~ Number of expected clusters.\n",
    "    # theta ~ List object container with mixing component \"phi\" and cluster centroids \"mu\".\n",
    "    # gamma ~ posterior estimates for p(Z|X,thetaOld)\n",
    "    # axis ~ Indicate whether the unique words are given rowwise or column wise. 1 Indicates rowwise. \n",
    "\n",
    "    # RETURN #\n",
    "    ##########\n",
    "    # Theta with new mu (word proportion parameters) and phi(mixing parameters)\n",
    "\n",
    "    # recast feature vector to local variable.\n",
    "    fv_ <- featureVector;\n",
    "\n",
    "    N <- dim(fv_)[2] # number of documents.\n",
    "    W <- dim(fv_)[1] # number of words.\n",
    "    K <- K # number of clusters.\n",
    "\n",
    "    # BEGIN #\n",
    "    #########\n",
    "    for (k in 1:K) {\n",
    "        # for each K update the new mixing components.\n",
    "        theta$phi[k] <- sum(gamma[, k] / N);\n",
    "\n",
    "        # for each document update the word proportion parameter mu_k_w\n",
    "        for (w in 1:W) {\n",
    "            # for each word find how many times it occures in the documents belonging to current cluster K.\n",
    "            theta$mu[k,w] <- sum(gamma[, k] * fv_[w,])/sum(fv_[w,]);\n",
    "\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return(theta);\n",
    "}\n",
    "\n",
    "intern_thetaInit <- function(nWords,K=4,seed=1234) {\n",
    "\n",
    "    # This function initialises theta for document clustering. This includes phi (AKA rho), and mu.\n",
    "\n",
    "    # ARGS #\n",
    "    ########\n",
    "\n",
    "    # nWords ~ Length of number of unique words as integer.\n",
    "    # K ~ Number of expected clusters as integer.\n",
    "    # seed ~ Reproducibility, set to NULL if not required.\n",
    "\n",
    "    # RETURN #\n",
    "    ##########\n",
    "    # List object with matrices for phi (mixing components) and mu (word proportion parameters)\n",
    "\n",
    "    if (!is.null(seed)) {\n",
    "        set.seed(seed);\n",
    "    }\n",
    "\n",
    "    # initialise phi(AKA rho)\n",
    "    phi_ <- matrix(1 / K, nrow = K, ncol = 1);\n",
    "\n",
    "    # initalise mu randomly\n",
    "    mu_ <- matrix(runif(K * nWords), nrow = K, ncol = nWords);\n",
    "    # normalise mu [0,1]\n",
    "    mu_ <- prop.table(mu_, 1);\n",
    "\n",
    "\n",
    "    return(list(\"phi\" = phi_, \"mu\" = mu_));\n",
    "}\n",
    "\n",
    "\n",
    "dClust_eMax <- function(FeatureVector, K = 4,iterMax=10, hard = FALSE,seed=1234) {\n",
    "\n",
    "    # This function takes a CountFeatureVector from text preprocessing and clusters documents using Expectation Maximisation.\n",
    "    \n",
    "    # ARGS #\n",
    "    ########\n",
    "    # FeatureVector ~ Count / TF-IDF vector as matrix.\n",
    "    # K ~ Number of expected clusters.\n",
    "    # hard ~ Whether the cluster assignment is probabilistic or absolute. If hard = TRUE, then cluster assignments are made by assignining the cluster of the highest probability of generation. i.e. argmax ln(p(Z|X,theta)).\n",
    "    # axis ~ Indicate whether the unique words are given rowwise or column wise. 1 Indicates rowwise. \n",
    "\n",
    "\n",
    "\n",
    "    # RETURN #\n",
    "    ##########\n",
    "    # Returns an object containing cluster and other items.\n",
    "\n",
    "\n",
    "    # recast feature vector to local variable.\n",
    "    fv_ <- FeatureVector;\n",
    "\n",
    "    # get number of documents and number of unique words\n",
    "\n",
    "        nDocs_ <- dim(fv_)[2] # number of documents.\n",
    "        nWords_ <- dim(fv_)[1] # number of unique words.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # INITIALISATION #\n",
    "    ##################\n",
    "\n",
    "    # initialise parameters using parameter initialisation function.\n",
    "    theta_ <- intern_thetaInit(nWords = nWords_, K, seed);\n",
    "\n",
    "    # initialise gamma for posterior probabilities that each document belongs to cluster K. \n",
    "    gamma_ <- matrix(0, nrow = nDocs_, ncol = K);\n",
    "\n",
    "\n",
    "    # MAIN ITERATION #\n",
    "    ##################\n",
    "\n",
    "    for (i in 1:iterMax) {\n",
    "\n",
    "        gamma_ <- intern_expectation(fv_, K, theta_, gamma_);\n",
    "        theta_ <- intern_maximisation(fv_, gamma_, theta_, K);\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    return(list(\"gamma\"=gamma_,\"theta\"=theta_));\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii.\n",
    "Load Task3A.text file and necessary libraries (if needed, perform text preprocessing similar to what we did in Activity 4.2), set the number of clusters K=4, and run both the soft-EM and hard-EM algorithms on the provided data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: NLP\n"
     ]
    }
   ],
   "source": [
    "data <- read.data(file.name = './Task3A.txt', sample.size = 0, seed = 100, pre.proc = TRUE, spr.ratio = .99)\n",
    "\n",
    "counts <- data$word.doc.mat\n",
    "\n",
    "res_ <- dClust_eMax(counts)\n",
    "\n",
    "label.hat <- apply(res_$gamma, 1, which.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iv.\n",
    "\n",
    "#### I couldn't get it to work from above, but i'll try to revisit this component at the end of this course since EM for GMMs is such an important tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
